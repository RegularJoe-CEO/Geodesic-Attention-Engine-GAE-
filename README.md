# Geodesic-Attention-Engine-GAE-
Geodesic Attention Engine - Minimum-energy path through transformer attention. Fused Waller Operator reduces HBM round-trips from 12 to 2. O(N) memory complexity, 23-37% Tok/J improvement, bit-exact determinism. No approximation, no sparsity - just the shortest path.
